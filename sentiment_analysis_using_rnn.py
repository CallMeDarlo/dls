# -*- coding: utf-8 -*-
"""Sentiment Analysis Using RNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_i2L5XZavdlCZCyPqHLEnSZirrU3oB4
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import imdb

# Load the IMDB dataset
vocab_size = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# Combine train and test sets for analysis
x = np.concatenate((x_train, x_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)

# Dataset Overview
print("Shape of x_train:", x_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of x_test:", x_test.shape)
print("Shape of y_test:", y_test.shape)

# Class Distribution
unique, counts = np.unique(y, return_counts=True)
class_distribution = dict(zip(unique, counts))
print("Class Distribution:", class_distribution)

# Text Length Distribution
review_lengths = [len(review) for review in x]
print("Average Length of Reviews:", np.mean(review_lengths))
print("Minimum Length of Reviews:", np.min(review_lengths))
print("Maximum Length of Reviews:", np.max(review_lengths))

plt.figure(figsize=(8, 6))
plt.hist(review_lengths, bins=50, color='skyblue', edgecolor='black')
plt.title('Text Length Distribution')
plt.xlabel('Length of Reviews')
plt.ylabel('Frequency')
plt.show()

# Word Frequency Analysis
word_counts = {}
for review in x:
    for word in review:
        if word not in word_counts:
            word_counts[word] = 1
        else:
            word_counts[word] += 1

sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
top_words = sorted_word_counts[:20]

print("Top 20 Most Common Words:")
for word, count in top_words:
    print(f"{word}: {count}")

# Sentiment Analysis
positive_reviews = x[y == 1]
negative_reviews = x[y == 0]

print("Number of Positive Reviews:", len(positive_reviews))
print("Number of Negative Reviews:", len(negative_reviews))



# Implementation using RNN
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# Load the IMDB dataset
vocab_size = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# Pad sequences to ensure uniform length
max_length = 200
x_train = pad_sequences(x_train, maxlen=max_length)
x_test = pad_sequences(x_test, maxlen=max_length)

# Define RNN model
embedding_dim = 16
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    SimpleRNN(units=64),
    Dense(units=1, activation='sigmoid')
])

# Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential

vocab_size = 10000  # Example vocabulary size
embedding_dim = 128  # Example embedding dimension
maxlen = 100  # Example maximum length of input sequences
lstm_units = 64  # Example number of LSTM units

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))
model.add(LSTM(lstm_units))
model.add(Dense(1, activation='sigmoid'))